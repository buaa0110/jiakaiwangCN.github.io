<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Jiakai Wang (王嘉凯)</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
<!--     <link rel="icon" type="image/jpg" href="https://htqin.github.io/buaa_icon.jpg"> -->
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Jiakai Wang (王嘉凯)</name></p>
                <p align="justify">I am now a Assitant Researcher in <strong>ZGC Lab</strong>, Beijing, China. I received the Ph.D. degree in 2022 from <a href="https://www.buaa.edu.cn/">Beihang University</a> (<i>Summa Cum Laude</i>), supervised by Prof. <a href="http://sites.nlsde.buaa.edu.cn/~liwei/">Wei Li</a> and Prof. <a href="http://sites.nlsde.buaa.edu.cn/~xlliu/">Xianglong Liu</a>. Before that, I obtained my BSc degree in 2018 from <a href="https://www.buaa.edu.cn/">Beihang University</a> (<i>Summa Cum Laude</i>).
                    <br><br>
<!--                     In my PhD study, I interned at the WeiXin Group (WXG) of <a href="https://www.tencent.com/en-us">Tencent</a> (Shenzhen, China). 
                    In my undergraduate study, I interned at the Speech Group of Microsoft Research Asia (<a href="https://www.msra.cn/">MSRA</a>) (Beijing,
                    China) and the Big Data Intelligence Group on SmartCity (<a href="https://www.bigscity.com/">BIGSCity</a>) of Beihang University (Beijing, China).
                    <br><br> -->
                    <strong>Email:</strong> jk_buss_scse@buaa.edu.cn
                <br>

                </p><p align="center">
                    <a href="https://github.com/buaa0110"> Github </a> / 
                    <a href="https://www.linkedin.com/in/N2S/">Linkedin</a> / 
                    <a href="https://scholar.google.com/citations?user=RoFr1qcAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> / 
<!--                     <a href="https://www.zhihu.com/column/c_1343181023409074176">知乎专栏</a> -->
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">
		    My research interest is <strong>Trustworthy AI</strong> in <strong>Computer Vision</strong>, which consists of the physical adversarial examples generation, adversarial defense and evaluation. I hold the review that physical adversarial attacks and defenses can powerfully promote the development of secure and robust artificial intelligence, leading to a healthier future society.
            </p>
            Now my research mainly includes:
                <ul>
                    <li>
                        Adversarial examples generation
                    </li>
                    <li>
                        Defend adversarial attacks in the physical world
                    </li>
                    <li>
                        3D adversarial attack
                    </li>
                    <li>
                        Model robustness evaluation and testing
                    </li>
                </ul>
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading> 
            <p style="font-size:13px">⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐<strong><font color="red">[2022.06]I got my Ph.D. degree!!!!!!</font>⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐</strong></p>

            <p style="font-size:13px"> <strong><font color="red">[Challenge@CVPR2022]</font></strong> I am co-organizing the Challenge on <a href="https://aisafety.sensetime.com/">Robust Models towards Open-world Classification</a> Challenge at CVPR 2022. Please participate and win the $15K prize!</p>
              
            <p style="font-size:13px"> <strong><font color="red">[Workshop@CVPR2022]</font></strong> I am co-organizing the Workshop on <a href="https://artofrobust.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> Workshop&amp;Challenge at CVPR 2022. Please submit your papers and win the prizes!</p>
              
            <p style="font-size:13px"> <strong>[2022.03]</strong> One paper accepted by CVPR 2022.</p>
              
            <p style="font-size:13px"> <strong>[2021.10]</strong> One paper accepted by IEEE TIP.</p>

            <p> <strong>[2021.08.21]</strong> One co-authored paper</a> for scene text recognition is accepted by <a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation">Elsevier JVCI</a>.

            <p> <strong>[2021.08.12]</strong> One <a href="">paper</a> for deepfake detection is accepted by <a href="https://advm-workshop-2021.github.io/">ACM MM 2021 workshop: Adversarial Learning for Multimedia</a>.
                
            </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Selected Publication</heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>

        <tr><td width="20%"><img src="./imgs/CVPR2022-1.png" alt="CVPR 2022" width="180" height = "160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2204.06213">
                 <papertitle>Defensive Patches for Robust Recognition in the Physical World</papertitle></a>
                 <br><strong>Jiakai Wang</strong>, Zixin Yin, Pengfei Hu, Renshuai Tao, Haotong Qin, Xianglong Liu, Dacheng Tao, Aishan Liu.<br>
                 <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
				 <br>
                 <a href="https://arxiv.org/abs/2204.06213">pdf</a>  /
				<a href="https://github.com/nlsde-safety-team/DefensivePatch"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=nlsde-safety-team&repo=DefensivePatch&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
               
                 <p align="justify" style="font-size:13px"> We generate defensive patches to help building robust recognition systems in practice against noises by simply sticking them on the target object.</p>
                <p></p>
            </td>
        </tr>
        <tr><td width="20%"><img src="./imgs/aco-tip.png" alt="TIP2021" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://ieeexplore.ieee.org/document/9462815">
                 <papertitle>Universal Adversarial Patch Attack for Automatic Checkout using Perceptual and Attentional Bias</papertitle></a>
                 <br><strong>Jiakai Wang*</strong>, Aishan Liu*,  Xiao Bai, Xianglong Liu <br><br>
                 <em>IEEE Transactions on Image Processing (TIP)</em>, 2021
				 <font color="red"><strong>(IF=10.86)</strong></font>
                 <br>
                 <a href="https://ieeexplore.ieee.org/document/9462815">pdf</a>
				 / <a href="https://github.com/nlsde-safety-team/PerceptualAttentionalBiasedAttack"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=nlsde-safety-team&repo=PerceptualAttentionalBiasedAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>

                 <p align="justify" style="font-size:13px"> We propose a bias-based framework to generate universal adversarial patches with strong generalization ability, which exploits the perceptual bias and attentional bias to improve the attacking ability.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/dualattention.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/pdf/2103.01050.pdf">
                    <papertitle>Dual Attention Suppression Attack: Generate Adversarial Camouflage in Physical World</papertitle></a>
                    <br><strong>Jiakai Wang</strong>, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, Xianglong Liu.<br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                    <font color="red"><strong>(Oral)</strong></font>
                    <br>
                    <a href="https://arxiv.org/pdf/2103.01050.pdf">pdf</a> /
                   <font color="red"> News:</font>
                   <a href="https://mp.weixin.qq.com/s/cIcJvmkbvQk-W_qJADkSqw"><font color="red">(机器之心)</font></a>
                   /<a href="https://github.com/nlsde-safety-team/DualAttentionAttack"><font color="red">Project page</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=nlsde-safety-team&repo=DualAttentionAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                   
                    <p align="justify" style="font-size:13px">We propose the Dual Attention Suppression (DAS) attack to generate visually-natural physical adversarial camouflages with strong transferability by suppressing both model and human attention. </p>
                   <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/ECCV_2.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/pdf/2005.09257.pdf">
                    <papertitle>Bias-based Universal Adversarial Patch Attack for Automatic Check-out</papertitle></a>
                    <br>Aishan Liu*, <strong>Jiakai Wang*</strong>, Xianglong Liu, Bowen Cao, Chongzhi Zhang, Hang Yu.<br>
                    <em>European Conference on Computer Vision (ECCV)</em>, 2020
                    <br>
                    <a href="https://arxiv.org/pdf/2005.09257.pdf">pdf</a> /
                   <font color="red"> News:</font>
                   <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652073635&idx=5&sn=b1acd091996cacb9e74053c4208b793c&chksm=f1201a52c6579344ae75ccb2ee3042ed3eddbb6bd988000c7b1ba8c274f1aceaec7ea1300d1d&mpshare=1&scene=1&srcid=07082kvhcWURQF4VRcIx8uU9&sharer_sharetime=1596855924325&sharer_shareid=da9c9379a79901c18dc93793609d62fa&key=4defdd0e8978fadbc4f7d3467b572eb060d5482035de4befee54b935c6aabbcaafa3ed60343840f82abb27fbcc57798b93e6f215c44f11a37e87141722c58b1167ffeba220d7150c5c8ee9333d06f513&ascene=1&uin=MTQzMTA0NDAw&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=AfGxCG3P9erzoZeJcwLMjbg%3D&pass_ticket=qME0ljezjearOlDwgFgo%2F6ZH0VZ%2B7CLScg%2FNCc5rqMk%3D"><font color="red">(新智元)</font></a>
                   /<a href="https://github.com/nlsde-safety-team/PerceptualAttentionalBiasedAttack"><font color="red">Project page</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=liuaishan&repo=ModelBiasedAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                   
                    <p align="justify" style="font-size:13px">We propose a bias-based framework to generate class-agnostic universal adversarial patches with strong generalization ability, which exploits both the perceptual and semantic bias of models.  </p>
                   <p></p>
            </td>
        </tr>
			
	
        <td width="20%"><img src="./imgs/Eval1.jpg" alt="AIView" width="180" height = "110" style="border-style: none"></td>
           <td width="80%" valign="top">
                <p><a href="http://www.cesi.ac.cn/202007/6566.html">
                <papertitle>人工智能机器学习模型及系统的质量要素和测试方法</papertitle></a>
                <br> <strong>王嘉凯</strong>, 刘艾杉, 刘祥龙<br>
                <em>信息技术与标准化</em>, 2020
                
                <br>
                <a href="http://www.cesi.ac.cn/202007/6566.html">pdf</a> 
               <p></p>
           </td>
        </tr>

	

        </tbody>
    </table>
	
     <!--Thesis  -->
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tbody><tr>
          <td><heading>Highlight Project</heading>
          </tr></tbody>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/Chongming.jpg" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                 <papertitle>重明 (AISafety)</papertitle></a>
                 <br>
                  <br>
                 <a href="https://arxiv.org/pdf/2101.09617.pdf">pdf</a> /
				  <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml"><font color="red"> (News: TechWeb)</font></a> /
				 <a href="https://github.com/DIG-Beihang/AISafety"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=AISafety&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                
                 <p align="justify" style="font-size:13px">重明 is an open-source platform to evaluate model robustness and safety towards noises (e.g., adversarial examples, corruptions, etc.). 
				 The name is taken from the Chinese myth <a href="https://baike.baidu.com/item/%E9%87%8D%E6%98%8E%E9%B8%9F/5482222?fr=aladdin">重明鸟</a>, which has strong power, could fight against beasts and avoid disasters. 
				 We hope our platform could improve the robustness of deep learning systems and help them to avoid safety-related problems. 
				 重明 has been awarded the <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml">首届OpenI启智社区优秀开源项目</a> (First OpenI Excellent Open Source Project).
                 </p>
                <p></p>
            </td>
        </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
            <td width="20%"><img src="./imgs/robustart.png" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
                <td width="80%" valign="top">
                     <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                     <papertitle>RobustART</papertitle></a>
                     <br>
                      <br>
                     <a href="https://arxiv.org/pdf/2109.05211.pdf">pdf</a> /
                      <a href="https://baijiahao.baidu.com/s?id=1711221498985379616&wfr=spider&for=pc"><font color="red"> (News: 机器之心)</font></a> /
                     <a href="http://robust.art/"><font color="red">Project page</font></a>
                     <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=RobustART&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                    
                    
                     <p align="justify" style="font-size:13px">RobustART is the first comprehensive Robustness investigation benchmark on large-scale dataset ImageNet regarding ARchitectural design (49 human-designed off-the-shelf architectures and 1200+ neural architecture searched networks) and Training techniques (10+ general ones e.g., extra training data, etc) towards diverse noises (adversarial, natural, and system noises). 
                     Our benchmark (including open-source toolkit, pre-trained model zoo, datasets, and analyses): 
                     (1) presents an open-source platform for conducting comprehensive evaluation on diverse robustness types; (2) provides a variety of pre-trained models with different training techniques to facilitate robustness evaluation; (3) proposes a new view to better understand the mechanism towards designing robust DNN architectures, backed up by the analysis. 
                     We will continuously contribute to building this ecosystem for the community.
                     </p>
                    <p></p>
                </td>
            </tr>
    
            </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
                <td><heading>Academic Services</heading>
                <p style="font-size:13px"> <strong>[2022]</strong> Co-organizer of the Workshop on <a href="https://practical-dl.github.io/">International Workshop on The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> at CVPR 2022. </p>
                <p><strong>[2022]</strong> ACM MM, ECCV, CVPR, Reviewer.</p>
                <p style="font-size:13px"> <strong>[2021]</strong> Co-organizer of the Forum on Safety and Privacy for Multimedia Systems at <a href="https://conf.ccf.org.cn/web/html7/TYMB.html?channelId=8a9e362c7b9bc357017ba11235630029&superChannel=8a9e362c7b9bc357017ba0fae29f0017&globalId=m8341723535535022081618923787260">ChinaMM 2021</a>. </p>        
                <p><strong>[2021]</strong> ACM MM, Pattern Recognition, IET Image Processing Reviewer.</p>
               </td>
               </tr></tbody>
       </table>


     <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Main Awards</heading>
            <p> <strong>[2022.06]</strong> &nbsp;&nbsp; Outstanding Graduates of Beijing Province</p>
            <p> <strong>[2022.01]</strong> &nbsp;&nbsp; Beihang University Exploring Scholarship.</p>
            <p> <strong>[2021.10]</strong> &nbsp;&nbsp; Beihang University Guorui Scholarship.</p>
            <p> <strong>[2021.10]</strong> &nbsp;&nbsp; Beihang University Merit Student.</p>
            <p> <strong>[2021.06]</strong> &nbsp;&nbsp; Beihang University First Prize Scholarship.</p>
	        <p> <strong>[2021.06]</strong> &nbsp;&nbsp; Beihang University Excellent Academic Paper Award.</p>
	        <p> <strong>[2020.10]</strong> &nbsp;&nbsp; Beihang University First Prize Scholarship.</p>
            <p> <strong>[2020.09]</strong> &nbsp;&nbsp; China National Scholarship (<strong>Top2%</strong>).</p>
            <p> <strong>[2020.09]</strong> &nbsp;&nbsp; Beihang University Merit Student.</p>
            <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Beihang University First Prize Scholarship.</p>
            <p> <strong>[2018.09]</strong> &nbsp;&nbsp; Beihang University Outstanding Freshman Scholarship (<strong>1/12</strong>).</p>
            <p> <strong>[2018.06]</strong> &nbsp;&nbsp; Outstanding Graduates of Beijing Province.</p>
           </td>
           </tr></tbody>
   </table>


    <!--SECTION 9 -->
    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=F8qBcfOz5fEukF2GbSvTknfvOhlRh0N-pdOTCXWEHtk"></script>
    </p></td>
    </tr>
    </tbody>
    </table>
